{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../Picture Data/logo.png\" alt=\"Header\" style=\"width: 800px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@Copyright (C): 2010-2019, Shenzhen Yahboom Tech  \n",
    "@Author: Malloy.Yuan  \n",
    "@Date: 2019-07-17 10:10:02  \n",
    "@LastEditors: Malloy.Yuan  \n",
    "@LastEditTime: 2019-09-17 17:54:19  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autopilot--Pedestrian detects parking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model\n",
    "\n",
    "We assume that you have trained ``best_steering_model_xy.pth`` to the example folder in accordance with the instructions in the autopilot training model example.\n",
    "\n",
    "> ### Execute the code below that we should be familiar with to initialize the PyTorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "\n",
    "model = torchvision.models.resnet18(pretrained=False)\n",
    "model.fc = torch.nn.Linear(512, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from servoserial import ServoSerial\n",
    "import threading\n",
    "# Kill pthread\n",
    "import inspect\n",
    "import ctypes\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "import time\n",
    "\n",
    "controller = widgets.Controller(index=0)\n",
    "display(controller)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the example of the object follower, the migration object detection function is demonstrated in this instance function.\n",
    "\n",
    "Load object detection model\n",
    "\n",
    "Add related algorithm methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import ObjectDetector\n",
    "global object_model\n",
    "object_model = ObjectDetector('ssd_mobilenet_v2_coco.engine')\n",
    "\n",
    "def detection_center(detection):\n",
    "    \"\"\"Calculate the center x, y coordinates of the object\"\"\"\n",
    "    bbox = detection['bbox']\n",
    "    center_x = (bbox[0] + bbox[2]) / 2.0 - 0.5\n",
    "    center_y = (bbox[1] + bbox[3]) / 2.0 - 0.5\n",
    "    return (center_x, center_y)\n",
    "\n",
    "def norm(vec):\n",
    "    \"\"\"Calculate the length of a two-dimensional vector\"\"\"\n",
    "    return np.sqrt(vec[0]**2 + vec[1]**2)\n",
    "\n",
    "def closest_detection(detections):\n",
    "    \"\"\"Find the detection closest to the center of the image\"\"\"\n",
    "    #Clear Cache\n",
    "    closest_detection = None\n",
    "    for det in detections:\n",
    "        center = detection_center(det)\n",
    "        if closest_detection is None:\n",
    "            closest_detection = det\n",
    "        elif norm(detection_center(det)) < norm(detection_center(closest_detection)):\n",
    "            closest_detection = det\n",
    "    return closest_detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, load the trained model from the ``best_steering_model_xy.pth`` file you uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best_steering_model_xy.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the model weights are on the CPU memory, then the following code is still passed to the GPU device as always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "model = model.to(device)\n",
    "model = model.eval().half()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a preprocessor function\n",
    "\n",
    "We have loaded our model now, but there is a small problem. The format of our training model does not exactly match the format of the camera.\n",
    "To do this, we need to do some pre-processing. This includes the following steps:\n",
    "\n",
    "1. Convert from HWC layout to CHW layout\n",
    "2. Normalize using the same parameters as we did during training (our camera provides values in the range [0, 255], and the training loaded image is in the range [0, 1], so we need to scale 255.0\n",
    "3. Transfer data from CPU memory to GPU memory\n",
    "4. Add a batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "\n",
    "mean = torch.Tensor([0.485, 0.456, 0.406]).cuda().half()\n",
    "std = torch.Tensor([0.229, 0.224, 0.225]).cuda().half()\n",
    "\n",
    "def preprocess(image):\n",
    "    image = PIL.Image.fromarray(image)\n",
    "    image = transforms.functional.to_tensor(image).to(device).half()\n",
    "    image.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
    "    return image[None, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have defined a preprocessing function that converts images from camera format to neural network input format.\n",
    "\n",
    "Now, let's start showing our camera. After going through the last few examples, you should be familiar with this now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "# import ipywidgets\n",
    "import ipywidgets.widgets as widgets\n",
    "import traitlets\n",
    "from jetbot import Camera, bgr8_to_jpeg\n",
    "from servoserial import ServoSerial\n",
    "\n",
    "# camera = Camera()\n",
    "camera = Camera.instance(width=300, height=300)\n",
    "servo_device = ServoSerial() \n",
    "\n",
    "# image_widget = ipywidgets.Image()\n",
    "# traitlets.dlink((camera, 'value'), (image_widget, 'value'), transform=bgr8_to_jpeg)\n",
    "# display(image_widget)\n",
    "\n",
    "image_widget = widgets.Image(format='jpeg', width=300, height=300)\n",
    "display(image_widget)\n",
    "\n",
    "\n",
    "def camservoInitFunction():\n",
    "    global leftrightpulse, updownpulse\n",
    "    leftrightpulse = 2048\n",
    "    updownpulse = 2048\n",
    "    servo_device.Servo_serial_control(1, 2048)\n",
    "    time.sleep(0.1)\n",
    "    servo_device.Servo_serial_control(2, 1300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following code to print out the detected object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections = object_model(camera.value)\n",
    "print(detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a robot instance to drive the motor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Robot\n",
    "\n",
    "robot = Robot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the slider to control the JetBot\n",
    "> ### Tip: We have configured initial values for the sliders. These initial values apply to our official Yahboom map, but if you want to train\n",
    "These values may not apply to your dataset when you are on your own different road map, so increase or decrease the slider depending on your settings and environment.\n",
    "\n",
    "1. Speed control (speed_gain_slider): To start JetBot, add ``speed_gain_slider``\n",
    "2. Steering gain control (steering_gain_sloder): If you see that the JetBot is spinning, you need to reduce ``steering_gain_slider`` until it becomes smooth\n",
    "3. Steering bias control (steering_bias_slider): If you see the JetBot biased to the far right or extreme left of the track, you should control this slider until the JetBot starts tracking the line or track at the center. This explains the motion deviation and camera offset\n",
    "\n",
    "> ## Note: When you slide the related slider mentioned above, in order to obtain a smooth JetBot road following behavior, the slider value should not be moved very quickly. The slider value should be moved gently to adjust the motion parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "speed_gain_slider = ipywidgets.FloatSlider(min=0.0, max=1.0, step=0.01, value=0.51,description='speed gain')\n",
    "steering_gain_slider = ipywidgets.FloatSlider(min=0.0, max=1.0, step=0.01, value=0.25, description='steering gain')\n",
    "steering_dgain_slider = ipywidgets.FloatSlider(min=0.0, max=0.5, step=0.001, value=0.24, description='steering kd')\n",
    "steering_bias_slider = ipywidgets.FloatSlider(min=-0.3, max=0.3, step=0.01, value=0, description='steering bias')\n",
    "\n",
    "display(speed_gain_slider, steering_gain_slider, steering_dgain_slider, steering_bias_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Base speed))speed_gain_slider     -> 0.51(Outer corner)   0.60(Inner curve)    0.42    0.56\n",
    "\n",
    "(P)steering_gain_slider          -> 0.25           0.37            0.22    0.27\n",
    "\n",
    "(D)steering_dgain_slider         -> 0.24           0.24            0.10    0.13\n",
    "\n",
    "(Base steering value)steering_bias_slider -> 0.00           0.00            0.00    0.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The x and y sliders will display the predicted x, y values.\n",
    "\n",
    "The steering slider will display our estimated steering value. This value is not the actual angle of the target, but a proportional value. When the actual angle is ``0``, this is 0, which increases/decreases as the actual angle increases/decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_slider = ipywidgets.FloatSlider(min=-1.0, max=1.0, description='x')\n",
    "y_slider = ipywidgets.FloatSlider(min=0, max=1.0, orientation='vertical', description='y')\n",
    "steering_slider = ipywidgets.FloatSlider(min=-1.0, max=1.0, description='steering')\n",
    "speed_slider = ipywidgets.FloatSlider(min=0, max=1.0, orientation='vertical', description='speed')\n",
    "\n",
    "display(ipywidgets.HBox([y_slider, speed_slider]))\n",
    "display(x_slider, steering_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that is called when the value of the camera changes. This function will perform the following steps\n",
    "\n",
    "1. Preprocess camera image\n",
    "2. Perform a neural network\n",
    "3. Calculate the approximate steering value\n",
    "4. Control the motor using proportional/differential control (PD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle = 0.0\n",
    "angle_last = 0.0\n",
    "\n",
    "def execute(change):\n",
    "    global angle, angle_last\n",
    "    image = change['new']\n",
    "    \n",
    "    # Calculate all detected objects\n",
    "    detections = object_model(image)\n",
    "    \n",
    "    # Draw all detected objects\n",
    "#     for det in detections[0]:\n",
    "#         bbox = det['bbox']\n",
    "#         cv2.rectangle(image, (int(300 * bbox[0]), int(300 * bbox[1])), (int(300 * bbox[2]), int(300 * bbox[3])), (255, 0, 0), 2)\n",
    "    \n",
    "    #Select the object you want to track, select the value of label_widget, 1 is person\n",
    "    # select detections that match selected class label\n",
    "    matching_detections = [d for d in detections[0] if d['label'] == 1]\n",
    "    \n",
    "    #Mark the object to be tracked with green lines.\n",
    "    # get detection closest to center of field of view and draw it\n",
    "    det = closest_detection(matching_detections)\n",
    "    if det is not None:\n",
    "#         bbox = det['bbox']\n",
    "#         cv2.rectangle(image, (int(300 * bbox[0]), int(300 * bbox[1])), (int(300 * bbox[2]), int(300 * bbox[3])), (0, 255, 0), 4)\n",
    "        ''' Stop the current Jetbot movement if it detects an object on the road that needs to be avoided '''\n",
    "        robot.stop()\n",
    "    else:\n",
    "        xy = model(preprocess(image)).detach().float().cpu().numpy().flatten()\n",
    "        x = xy[0]\n",
    "        y = (0.5 - xy[1]) / 2.0\n",
    "\n",
    "        x_slider.value = x\n",
    "        y_slider.value = y\n",
    "\n",
    "        speed_slider.value = speed_gain_slider.value\n",
    "\n",
    "        angle = np.arctan2(x, y)\n",
    "        pid = angle * steering_gain_slider.value + (angle - angle_last) * steering_dgain_slider.value\n",
    "        angle_last = angle\n",
    "\n",
    "        steering_slider.value = pid + steering_bias_slider.value\n",
    "\n",
    "        #PID + Base speed + Gain\n",
    "        robot.left_motor.value = max(min(speed_slider.value + steering_slider.value, 1.0), 0.0)\n",
    "        robot.right_motor.value = max(min(speed_slider.value - steering_slider.value, 1.0), 0.0)\n",
    "    \n",
    "    \n",
    "    # update image widget\n",
    "    image_widget.value = bgr8_to_jpeg(image)\n",
    "execute({'new': camera.value})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created our neural network execution function, but now we need to attach it to the camera for processing.\n",
    "\n",
    "> ## Tip: This code will move the robot!! Please put the Jetbot robot on the map you have trained before. If you collect the data and the model is well trained, you can see that Jetbot runs smoothly on the road. !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camservoInitFunction()\n",
    "camera.observe(execute, names='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your Jetbot functions properly, it should now generate new commands for each new camera frame.\n",
    "\n",
    "Now, you can put the JetBot on a track that has collected data and see if it can track the track.\n",
    "\n",
    "If you want to stop this behavior, you can unload the binding of this callback function by executing the code for the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.unobserve(execute, names='value')\n",
    "time.sleep(0.1)\n",
    "robot.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jetbot_motion():\n",
    "    count1 = count2 = count3 = count4 =  count5 = 0\n",
    "    while 1:\n",
    "        #Robot car left and right DC motor\n",
    "        if controller.axes[1].value <= 0.1:\n",
    "            if (controller.axes[0].value <= 0.1 and controller.axes[0].value >= -0.1 \n",
    "                and controller.axes[1].value <= 0.1 and controller.axes[1].value >= -0.1):\n",
    "                robot.stop()\n",
    "            else:\n",
    "                robot.set_motors(-controller.axes[1].value + controller.axes[0].value, -controller.axes[1].value - controller.axes[0].value)\n",
    "            \n",
    "            time.sleep(0.01)\n",
    "        else:\n",
    "            robot.set_motors(-controller.axes[1].value - controller.axes[0].value, -controller.axes[1].value + controller.axes[0].value)\n",
    "            time.sleep(0.01)\n",
    "          #Handle control code---2(Xbox360手柄)\n",
    "#         if controller.axes[1].value <= 0:\n",
    "#             robot.set_motors(-controller.axes[1].value + controller.axes[0].value, -controller.axes[1].value - controller.axes[0].value)\n",
    "#             time.sleep(0.01)\n",
    "#         else:\n",
    "#             robot.set_motors(-controller.axes[1].value - controller.axes[0].value, -controller.axes[1].value + controller.axes[0].value)\n",
    "#             time.sleep(0.01)\n",
    "            \n",
    "# thread1 = threading.Thread(target=jetbot_motion)\n",
    "# thread1.setDaemon(False)\n",
    "# thread1.start()\n",
    "\n",
    "def _async_raise(tid, exctype):\n",
    "    \"\"\"raises the exception, performs cleanup if needed\"\"\"\n",
    "    tid = ctypes.c_long(tid)\n",
    "    if not inspect.isclass(exctype):\n",
    "        exctype = type(exctype)\n",
    "    res = ctypes.pythonapi.PyThreadState_SetAsyncExc(tid, ctypes.py_object(exctype))\n",
    "    if res == 0:\n",
    "        raise ValueError(\"invalid thread id\") \n",
    "    elif res != 1:\n",
    "        # \"\"\"if it returns a number greater than one, you're in trouble,\n",
    "        # and you should call it again with exc=NULL to revert the effect\"\"\"\n",
    "        ctypes.pythonapi.PyThreadState_SetAsyncExc(tid, None)\n",
    "        \n",
    "def stop_thread(thread):\n",
    "    _async_raise(thread.ident, SystemExit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread1 = threading.Thread(target=jetbot_motion)\n",
    "thread1.setDaemon(False)\n",
    "thread1.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_thread(thread1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "If your JetBot doesn't track the road well, try to find out where it failed. The beauty is that we can collect more data for these failure scenarios, and JetBot should do better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
