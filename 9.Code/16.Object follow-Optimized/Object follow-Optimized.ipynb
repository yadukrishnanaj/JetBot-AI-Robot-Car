{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../Picture Data/logo.png\" alt=\"Header\" style=\"width: 800px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@Copyright (C): 2010-2019, Shenzhen Yahboom Tech  \n",
    "@Author: Malloy.Yuan  \n",
    "@Date: 2019-07-17 10:10:02  \n",
    "@LastEditors: Malloy.Yuan  \n",
    "@LastEditTime: 2019-09-17 17:54:19  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object follow-Optimized\n",
    "\n",
    "In this course, we will show how to track objects using JetBot!\n",
    "We will use a pre-trained neural network trained on [COCO dataset] (http://cocodataset.org) to detect 90 different common objects. These include:\n",
    "\n",
    "* Person (index 0)\n",
    "* Cup (index 47)\n",
    "\n",
    "Import the \"ObjectDetector\" class, which uses our pre-trained SSD engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single camera image detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from jetbot import ObjectDetector\n",
    "\n",
    "model = ObjectDetector('ssd_mobilenet_v2_coco.engine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, the 'ObjectDetector' class uses the TensorRT Python API to execute the engine we provide. It is also responsible for pre-processing the input of the neural network and parsing the detected objects.\n",
    "\n",
    "Currently, it only works with engines created with the ``jetbotssd_tensorrt`` package. This package has utilities for converting models from the TensorFlow object detection API to the optimized TensorRT engine. Next, let's initialize the camera. Our detector needs 300x300 pixels of input, so we will set this parameter when creating the camera.\n",
    "\n",
    "> Internally, the Camera class uses GStreamer to take advantage of Jetson Nano's Image Signal Processor (ISP), and the speed at which images are captured is very impressive.\n",
    "\n",
    "> And also unloaded a lot of size calculations from the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Camera\n",
    "\n",
    "camera = Camera.instance(width=300, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's use some camera input to execute our network. By default, the ObjectDetector class expects the camera to generate a format of bgr8.\n",
    "\n",
    "However, if the input format is different, you can override the default preprocessor function. If there are any COCO objects in the camera's field of view, they should now be stored in the detections variable,\n",
    "\n",
    "# Display detected objects in the text area\n",
    "\n",
    "we print them out by the code shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections = model(camera.value)\n",
    "\n",
    "print(detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import ipywidgets.widgets as widgets\n",
    "\n",
    "detections_widget = widgets.Textarea()\n",
    "detections_widget.value = str(detections)\n",
    "display(detections_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the label, confidence, and border position of each object detected in each image. In this example there is only one image (our camera).\n",
    "\n",
    "To print the first object detected in the first image, we can call the following command\n",
    "\n",
    ">If no object detected, an error may be appear here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_number = 0\n",
    "object_number = 0\n",
    "\n",
    "print(detections[image_number][object_number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If no object detected, an error may be appear here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Control the robot to follow the center object\n",
    "\n",
    "Now, we want the robot to follow the object of the specified class. To do this, we will do the following work\n",
    "\n",
    "1. Detect objects that match the specified class\n",
    "2. Select the object closest to the center of the camera's field of view. This is the target object.\n",
    "3. Guide the robot to move to the target object, otherwise it will drift\n",
    "4. If robot car encounter obstacles, turn left\n",
    "We will also create widgets that control the target object label, robot speed and cornering gain, and control the speed of the robot's cornering based on the distance between the target object and the center of the robot's field of view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# collision_model = torchvision.models.alexnet(pretrained=False)\n",
    "# collision_model.classifier[6] = torch.nn.Linear(collision_model.classifier[6].in_features, 2)\n",
    "# collision_model.load_state_dict(torch.load('best_model.pth'))\n",
    "# device = torch.device('cuda')\n",
    "# collision_model = collision_model.to(device)\n",
    "\n",
    "mean = 255.0 * np.array([0.485, 0.456, 0.406])\n",
    "stdev = 255.0 * np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "normalize = torchvision.transforms.Normalize(mean, stdev)\n",
    "\n",
    "def preprocess(camera_value):\n",
    "    global device, normalize\n",
    "    x = camera_value\n",
    "    # Image zoomed to 224,224 versus 224,244 obstacle avoidance model\n",
    "    x = cv2.resize(x, (224, 224))\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    x = x.transpose((2, 0, 1))\n",
    "    x = torch.from_numpy(x).float()\n",
    "    x = normalize(x)\n",
    "    x = x.to(device)\n",
    "    x = x[None, ...]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a robot instance of the drive motor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Robot\n",
    "\n",
    "robot = Robot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the PID driver module, create a PID controller instance, and initialize the corresponding control variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PID\n",
    "\n",
    "global follow_speed\n",
    "follow_speed = 0.5\n",
    "global turn_gain\n",
    "turn_gain = 1.7\n",
    "global follow_speed_pid, follow_speed_pid_model\n",
    "follow_speed_pid_model = 1\n",
    "# follow_speed_pid = PID.PositionalPID(3, 0, 0)\n",
    "follow_speed_pid = PID.PositionalPID(1.5, 0, 0.05)\n",
    "global turn_gain_pid\n",
    "turn_gain_pid = PID.PositionalPID(0.15, 0, 0.05)\n",
    "global object_yservo_pid\n",
    "object_yservo_pid = PID.PositionalPID(3, 0.5, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we need to follow people, we can automatically adjust the angle of the PTZ to the elevation angle of Jetbot followers by the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from servoserial import ServoSerial\n",
    "servo_device = ServoSerial() \n",
    "# if label_widget.value == 1:\n",
    "servo_device.Servo_serial_double_control(1, 2048, 2, 2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We display all control widgets and connect the network execution function to the camera update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import bgr8_to_jpeg\n",
    "\n",
    "image_widget = widgets.Image(format='jpeg', width=300, height=300)\n",
    "label_widget = widgets.IntText(value=77, description='tracked label')\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([image_widget]),\n",
    "    label_widget,\n",
    "]))\n",
    "\n",
    "width = int(image_widget.width)\n",
    "height = int(image_widget.height)\n",
    "\n",
    "def detection_center(detection):\n",
    "    \"\"\"Calculate the center x, y coordinates of the object\"\"\"\n",
    "    bbox = detection['bbox']\n",
    "    center_x = (bbox[0] + bbox[2]) / 2.0 - 0.5\n",
    "    center_y = (bbox[1] + bbox[3]) / 2.0 - 0.5\n",
    "    return (center_x, center_y)\n",
    "    \n",
    "def norm(vec):\n",
    "    \"\"\"Calculate the length of a two-dimensional vector\"\"\"\n",
    "    return np.sqrt(vec[0]**2 + vec[1]**2)\n",
    "\n",
    "def closest_detection(detections):\n",
    "    \"\"\"Find the detection closest to the center of the image\"\"\"\n",
    "    closest_detection = None\n",
    "    for det in detections:\n",
    "        center = detection_center(det)\n",
    "        if closest_detection is None:\n",
    "            closest_detection = det\n",
    "        elif norm(detection_center(det)) < norm(detection_center(closest_detection)):\n",
    "            closest_detection = det\n",
    "    return closest_detection\n",
    "        \n",
    "def execute(change):\n",
    "    global follow_speed\n",
    "    global turn_gain\n",
    "    global follow_speed_pid\n",
    "    target_value_speed = 0\n",
    "    #Update image value\n",
    "    image = change['new']\n",
    "\n",
    "    # Execute a conflict model to determine if it is blocking\n",
    "    # execute collision model to determine if blocked\n",
    "    # collision_output = collision_model(preprocess(image)).detach().cpu()\n",
    "    # Apply the \"softmax\" function to normalize the output vector to have a sum of 1 (this makes it a probability distribution)\n",
    "    # prob_blocked = float(F.softmax(collision_output.flatten(), dim=0)[0])\n",
    "    # blocked_widget.value = prob_blocked\n",
    "    \n",
    "    # Blocking does not execute the following object detection program, directly return to the next round of data refresh\n",
    "    # turn left if blocked\n",
    "    # if prob_blocked > 0.5:\n",
    "    #     robot.left(0.3)\n",
    "    #     image_widget.value = bgr8_to_jpeg(image)\n",
    "    #     return\n",
    "        \n",
    "    # compute all detected objects\n",
    "    detections = model(image)\n",
    "    # First, mark the non-tracking objects that appear with blue lines.\n",
    "    # Draw all detections on image\n",
    "    for det in detections[0]:\n",
    "        bbox = det['bbox']\n",
    "        cv2.rectangle(image, (int(300 * bbox[0]), int(300 * bbox[1])), (int(300 * bbox[2]), int(300 * bbox[3])), (255, 0, 0), 2)\n",
    "    # Select the object you want to track, select the value of label_widget,1 is person,\n",
    "    # select detections that match selected class label\n",
    "    matching_detections = [d for d in detections[0] if d['label'] == int(label_widget.value)]\n",
    "    \n",
    "    # Then mark the object to be tracked with green lines.\n",
    "    # get detection closest to center of field of view and draw it\n",
    "    det = closest_detection(matching_detections)\n",
    "    if det is not None:\n",
    "        bbox = det['bbox']\n",
    "        cv2.rectangle(image, (int(300 * bbox[0]), int(300 * bbox[1])), (int(300 * bbox[2]), int(300 * bbox[3])), (0, 255, 0), 4)\n",
    "\n",
    "    # otherwise go forward if no target detected\n",
    "    '''\n",
    "    blob:https---------A string of tags generated by the blob object in html5 after being assigned to the video tag.\n",
    "    '''\n",
    "    if det is None:\n",
    "        # robot.forward(float(follow_speed))\n",
    "        robot.stop()\n",
    "    # otherwsie steer towards target\n",
    "    else:\n",
    "        # move robot forward and steer proportional target's x-distance from center\n",
    "        center = detection_center(det)\n",
    "\n",
    "        #Follow speed PID adjustment\n",
    "        follow_speed_pid.SystemOutput = 90000 * (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])\n",
    "        if label_widget.value == 1:\n",
    "            follow_speed_pid.SetStepSignal(30000)\n",
    "        elif label_widget.value == 53 or label_widget.value == 55: # 53:Apple 55:orange\n",
    "            follow_speed_pid.SetStepSignal(10000)\n",
    "        else:\n",
    "            follow_speed_pid.SetStepSignal(10000)\n",
    "        follow_speed_pid.SetInertiaTime(0.2, 0.1)\n",
    "\n",
    "        #Steering gain PID adjustment\n",
    "        turn_gain_pid.SystemOutput = center[0]\n",
    "        turn_gain_pid.SetStepSignal(0)\n",
    "        turn_gain_pid.SetInertiaTime(0.2, 0.1)\n",
    "\n",
    "        #Limit steering gain to the effective range\n",
    "        if label_widget.value == 1:\n",
    "            target_value_turn_gain = 0.6 + abs(turn_gain_pid.SystemOutput)\n",
    "        elif label_widget.value == 53 or label_widget.value == 55:\n",
    "            target_value_turn_gain = 0.5 + abs(turn_gain_pid.SystemOutput)\n",
    "        else:\n",
    "            target_value_turn_gain = 0.5 + abs(turn_gain_pid.SystemOutput)\n",
    "        if target_value_turn_gain < 0:\n",
    "            target_value_turn_gain = 0\n",
    "        elif target_value_turn_gain > 2:\n",
    "            target_value_turn_gain = 2\n",
    "\n",
    "        #Keep the output motor speed within the effective driving range\n",
    "        target_value_speed = 0.46 + follow_speed_pid.SystemOutput / 90000\n",
    "        target_value_speedl = target_value_speed + target_value_turn_gain * center[0]\n",
    "        target_value_speedr = target_value_speed - target_value_turn_gain * center[0]\n",
    "        if target_value_speedl<0.5:\n",
    "            target_value_speedl=0\n",
    "        elif target_value_speedl>1:\n",
    "            target_value_speedl = 1\n",
    "        if target_value_speedr<0.5:\n",
    "            target_value_speedr=0\n",
    "        elif target_value_speedr>1:\n",
    "            target_value_speedr = 1\n",
    "\n",
    "        robot.set_motors(target_value_speedl, target_value_speedr)\n",
    "        \n",
    "        if label_widget.value != 1:\n",
    "            #Vertical angle camera PID adjustment\n",
    "            object_yservo_pid.SystemOutput =bbox[1]*300+150*(bbox[3] - bbox[1])\n",
    "            # print('Y-axis object center position value:')\n",
    "            # print(object_yservo_pid.SystemOutput)\n",
    "            object_yservo_pid.SetStepSignal(150)\n",
    "            object_yservo_pid.SetInertiaTime(0.2, 0.1)\n",
    "            #Limit the vertical angle camera angle to the specified range\n",
    "            target_value_object_yservo = int(2048 + object_yservo_pid.SystemOutput)\n",
    "            servo_device.Servo_serial_control(2, target_value_object_yservo)\n",
    "    # Update image display to widget\n",
    "    image_widget.value = bgr8_to_jpeg(image)\n",
    "    \n",
    "execute({'new': camera.value})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the following code block to connect the execution function to each camera frame to update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.unobserve_all()\n",
    "camera.observe(execute, names='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the robot is not blocked, you can see that the blue box surrounds the detected object and the target object (the object that the robot follows) will be displayed in green.\n",
    "\n",
    "When the target is discovered, the robot should turn to the target.\n",
    "\n",
    "You can call the following code block to manually disconnect the camera and stop the robot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "camera.unobserve_all()\n",
    "time.sleep(1.0)\n",
    "robot.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
