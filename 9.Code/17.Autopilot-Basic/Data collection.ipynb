{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../Picture Data/logo.png\" alt=\"Header\" style=\"width: 800px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@Copyright (C): 2010-2019, Shenzhen Yahboom Tech  \n",
    "@Author: Malloy.Yuan  \n",
    "@Date: 2019-07-17 10:10:02  \n",
    "@LastEditors: Malloy.Yuan  \n",
    "@LastEditTime: 2019-09-17 17:54:19  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autopilot - Data collection\n",
    "\n",
    "If you have already browsed the collision avoidance example, you should be familiar with the following three steps.\n",
    "1.Data collection\n",
    "2. Training\n",
    "3. Deployment\n",
    "In this case, we will do the same thing. However, in addition to classification, you will learn another basic technique, 'regression'-regression, which we will use to enable Jetbot to follow a path (actually any path or target point).\n",
    "1. Place the Jetbot at different locations on the path (offset from the center, different angles, etc.).\n",
    "2. Display the live camera input from Robot 3.\n",
    "3. Using the gamepad controller, place a “green dot” on the image that corresponds to the target direction we want the robot to move.\n",
    "4. Store the X, Y values of this green dot along with the image of the robot camera.\n",
    "\n",
    "Then, in the training notebook, we will train a neural network to predict the X, Y values of our labels. In the live demo, we will use the predicted X, Y values to calculate an approximate steering value (it is not an \"exact\" angle, because this requires image calibration, but it is roughly proportional to the angle, so our controller Will work normally).\n",
    "So, how to determine the target position of this example?\n",
    "Here are some guidelines that we think might be helpful:\n",
    "1. Look at the live video of the camera.\n",
    "2. Imagine the path that the robot should follow (try to get close to the distance it needs to avoid running off the road, etc.).\n",
    "3. Place the target as far as possible so that the robot can rush directly to the target without “running away” from the road.\n",
    "\n",
    "> For example, if we are on a very straight road, we can put it on the horizon. If we are making a sharp turn, it may need to be placed closer to the robot so that it does not run out of the border.\n",
    "\n",
    "Assuming our deep learning model works as expected, these markup guidelines should ensure the following:\n",
    "1. The robot can move directly to the target safely (not out of bounds, etc.)\n",
    "2. The goal will continue to move along the path we imagined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data acquisition example video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/FW4En6LejhI\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import library module\n",
    "Import all the libraries needed for Data Collection.\n",
    "\n",
    "We will primarily use OpenCV to visualize and save images with labels. Uuid, datetime and other libraries are used for image naming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPython libraries be used to diplay and parts\n",
    "import traitlets\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "# Camera and servo interface foe Jetbot robot car\n",
    "from jetbot import Robot, Camera, bgr8_to_jpeg\n",
    "# Python base package for image annotations\n",
    "from uuid import uuid1\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import datetime\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "from servoserial import ServoSerial\n",
    "import threading\n",
    "# Kill pthread\n",
    "import inspect\n",
    "import ctypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display camera data in real time\n",
    "\n",
    "Our neural network takes an image of 224x224 pixels as input. We set the camera to this parameter to minimize the file size of the dataset (we have tested it for this task).\n",
    "\n",
    "In some scenarios, it's best to collect the data at a larger image size and then reduce it to the desired size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af606123f2bd4b2897b6d2dfdbed489f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abb83f4d525543ee8239cd5f61588c56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.0, description='x', max=1.0, min=-1.0, step=0.001)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f29525de1f3745abb3c45afec8795356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.0, description='y', max=1.0, min=-1.0, step=0.001)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "camera = Camera()\n",
    "\n",
    "image_widget = widgets.Image(format='jpeg', width=224, height=224)\n",
    "target_widget = widgets.Image(format='jpeg', width=224, height=224)\n",
    "\n",
    "x_slider = widgets.FloatSlider(min=-1.0, max=1.0, step=0.001, description='x')\n",
    "y_slider = widgets.FloatSlider(min=-1.0, max=1.0, step=0.001, description='y')\n",
    "\n",
    "def display_xy(camera_image):\n",
    "    image = np.copy(camera_image)\n",
    "    x = x_slider.value\n",
    "    y = y_slider.value\n",
    "    x = int(x * 224 / 2 + 112)\n",
    "    y = int(y * 224 / 2 + 112)\n",
    "    image = cv2.circle(image, (x, y), 8, (0, 255, 0), 3)\n",
    "    image = cv2.circle(image, (112, 224), 8, (0, 0,255), 3)\n",
    "    image = cv2.line(image, (x,y), (112,224), (255,0,0), 3)\n",
    "    jpeg_image = bgr8_to_jpeg(image)\n",
    "    return jpeg_image\n",
    "\n",
    "time.sleep(1)\n",
    "traitlets.dlink((camera, 'value'), (image_widget, 'value'), transform=bgr8_to_jpeg)\n",
    "traitlets.dlink((camera, 'value'), (target_widget, 'value'), transform=display_xy)\n",
    "\n",
    "display(widgets.HBox([image_widget, target_widget]), x_slider, y_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a handle controller instance\n",
    "\n",
    "Create an instance of the Controller widget, which we will use to mark the image with \"x\" and \"y\" values, as described in the introduction. The Controller widget accepts an index parameter that specifies the number of controllers. This is useful if you have multiple controllers, or if some gamepads appear as multiple controllers. To determine the index of the controller we are using, then before we create the handle instance we will follow the steps we have just learned to use the remote control handle:\n",
    "1. Visit http://html5gamepad.com\n",
    "2. Press the button on the gamepad you are using\n",
    "3. Remember the index of the gamepad that responds to the button\n",
    "\n",
    "Then, we will use this index to create and display the controller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cec2be5cb8d14d318cfd13de634c492c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Controller()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "controller = widgets.Controller(index=0)\n",
    "robot = Robot()\n",
    "display(controller)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect the Gamepad controller to the label image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widgets.jsdlink((controller.axes[2], 'value'), (x_slider, 'value'))\n",
    "widgets.jsdlink((controller.axes[3], 'value'), (y_slider, 'value'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection\n",
    "\n",
    "The code below will display the live image feed and the number of images we saved.\n",
    "\n",
    "We store the value of the target X, Y:\n",
    "\n",
    "1. Put the green dot on the target\n",
    "2. Press the 13th button to save\n",
    "\n",
    "Then the data we want will be saved to the ``dataset_xy`` folder. The saved file naming format is:\n",
    "``xy_<x value>_<y value>_<uuid>.jpg``\n",
    "When we train, we load the image and parse the x and y values in the file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = 'dataset_xy'\n",
    "\n",
    "# we have this \"try/except\" statement because these next functions can throw an error if the directories exist already\n",
    "try:\n",
    "    os.makedirs(DATASET_DIR)\n",
    "except FileExistsError:\n",
    "    print('Directories not created becasue they already exist')\n",
    "\n",
    "for b in controller.buttons:\n",
    "    b.unobserve_all()\n",
    "\n",
    "count_widget = widgets.IntText(description='count', value=len(glob.glob(os.path.join(DATASET_DIR, '*.jpg'))))\n",
    "\n",
    "def xy_uuid(x, y):\n",
    "    return 'xy_%03d_%03d_%s' % (x * 50 + 50, y * 50 + 50, uuid1())\n",
    "\n",
    "def save_snapshot(change):\n",
    "    if change['new']:\n",
    "        uuid = xy_uuid(x_slider.value, y_slider.value)\n",
    "        image_path = os.path.join(DATASET_DIR, uuid + '.jpg')\n",
    "        with open(image_path, 'wb') as f:\n",
    "            f.write(image_widget.value)\n",
    "        count_widget.value = len(glob.glob(os.path.join(DATASET_DIR, '*.jpg')))\n",
    "\n",
    "# Jetbot Yahboom Handle L side No.1 key save data\n",
    "controller.buttons[4].observe(save_snapshot, names='value')\n",
    "\n",
    "display(widgets.VBox([\n",
    "    target_widget,\n",
    "    count_widget\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ease to using, we can open a new thread to control the Jetbot robot car by handled to collect data through the handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jetbot_motion():\n",
    "    while 1:\n",
    "        #Robot car left and right DC motor\n",
    "        if controller.axes[1].value <= 0.1:\n",
    "            if (controller.axes[0].value <= 0.1 and controller.axes[0].value >= -0.1 \n",
    "                and controller.axes[1].value <= 0.1 and controller.axes[1].value >= -0.1):\n",
    "                robot.stop()\n",
    "            else:\n",
    "                robot.set_motors(-controller.axes[1].value + controller.axes[0].value, -controller.axes[1].value - controller.axes[0].value)\n",
    "            \n",
    "            time.sleep(0.01)\n",
    "        else:\n",
    "            robot.set_motors(-controller.axes[1].value - controller.axes[0].value, -controller.axes[1].value + controller.axes[0].value)\n",
    "            time.sleep(0.01)\n",
    "          #Handle control code---2(Xbox360 Handle)\n",
    "#         if controller.axes[1].value <= 0:\n",
    "#             robot.set_motors(-controller.axes[1].value + controller.axes[0].value, -controller.axes[1].value - controller.axes[0].value)\n",
    "#             time.sleep(0.01)\n",
    "#         else:\n",
    "#             robot.set_motors(-controller.axes[1].value - controller.axes[0].value, -controller.axes[1].value + controller.axes[0].value)\n",
    "#             time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start pthread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread1 = threading.Thread(target=jetbot_motion)\n",
    "thread1.setDaemon(True)\n",
    "thread1.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add method of close pthread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _async_raise(tid, exctype):\n",
    "    \"\"\"raises the exception, performs cleanup if needed\"\"\"\n",
    "    tid = ctypes.c_long(tid)\n",
    "    if not inspect.isclass(exctype):\n",
    "        exctype = type(exctype)\n",
    "    res = ctypes.pythonapi.PyThreadState_SetAsyncExc(tid, ctypes.py_object(exctype))\n",
    "    if res == 0:\n",
    "        raise ValueError(\"invalid thread id\") \n",
    "    elif res != 1:\n",
    "        # \"\"\"if it returns a number greater than one, you're in trouble,\n",
    "        # and you should call it again with exc=NULL to revert the effect\"\"\"\n",
    "        ctypes.pythonapi.PyThreadState_SetAsyncExc(tid, None)\n",
    "        \n",
    "\n",
    "def stop_thread(thread):\n",
    "    _async_raise(thread.ident, SystemExit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a method to adjust the position of the Jetbot to the autopilot angle and call this method to adjust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "servo_device = ServoSerial() \n",
    "def camservoInitFunction():\n",
    "    global leftrightpulse, updownpulse\n",
    "    leftrightpulse = 2048\n",
    "    updownpulse = 2048\n",
    "    servo_device.Servo_serial_control(1, 2048)\n",
    "    time.sleep(0.1)\n",
    "    servo_device.Servo_serial_control(2, 1300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the above method to adjust the pan/tilt to the autopilot angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camservoInitFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_thread(thread1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have collected enough data, we need to copy it to our GPU desktop or cloud machine for training. We can call the following terminal command to compress the dataset folder backup into a zip file.\n",
    "\n",
    "> # If you are training on JetBot, you can skip this step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The -r flag in the zip command indicates recursion, so we include all nested files, \n",
    "and the -q flag indicates silence, so the zip command will not print any output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestr():\n",
    "    return str(datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "\n",
    "!zip -r -q road_following_{DATASET_DIR}_{timestr()}.zip {DATASET_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After executing the above cell code, you should see a file called road_following_<Date&Time>.zip in the Jupyter file browser. \n",
    "\n",
    "You should download the zip file using the Jupyter file browser by right clicking and selecting download."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
